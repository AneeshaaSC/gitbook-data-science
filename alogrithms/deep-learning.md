# Deep Learning in General

* [Zayd's Blog – Why is machine learning 'hard'?](http://ai.stanford.edu/~zayd/why-is-machine-learning-hard.html?mkt_tok=eyJpIjoiTkRJeE0yTmpNak5sWVdRdyIsInQiOiJVVjI1S2RvcUFPRDcxcU9tN0xYZnU3NzhCT3M1XC9CRkdDb0VKbENFV2tqeWowWFVWTUNCd29GV0IwcVNqaDFnN2ErOVVLSHI0eHlEZlhGTHN1SnpiUUI5djJHajBDeUVWMGVIakZUZ1o4MEZjeHZhc1dXZ3lXV2NKVVkxZ2l4aDMifQ)

Neural nets really like standardized variables. \(per Jeremy\)

Softmax likes picking one class. So it is ridiculous to use it for multi-label classification \(per Jeremy\)

If a smaller batch size is used, the gradient is calculated using less number of images so it is less accurate as it is more volatile. You can try to re-run the learning rate finder to see if the best learning rate changed but it shouldn't make a huge difference as the learning rate differ exponentially. \(per Jeremy\)

[Improving the way we work with learning rate. – techburst](https://techburst.io/improving-the-way-we-work-with-learning-rate-5e99554f163b)

[The Cyclical Learning Rate technique // teleported.in](http://teleported.in/posts/cyclic-learning-rate/)

[Exploring Stochastic Gradient Descent with Restarts \(SGDR\)](https://medium.com/38th-street-studios/exploring-stochastic-gradient-descent-with-restarts-sgdr-fa206c38a74e)

[Notes from Coursera Deep Learning courses by Andrew Ng by Tess Ferrandez](https://www.dropbox.com/s/up1iifq2uvgg0zb/paper-180305120451.pdf?dl=0)

[Estimating an Optimal Learning Rate For a Deep Neural Network](https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0)



* [https://quid.com/feed/how-quid-uses-deep-learning-with-small-data](https://quid.com/feed/how-quid-uses-deep-learning-with-small-data) Recommended by Jeremey
* [https://medium.com/towards-data-science/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c](https://medium.com/towards-data-science/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c) Recommended by Jeremey
* [Three key learnings helped Merantix when applying deep learning to real-world problems](https://medium.com/merantix/applying-deep-learning-to-real-world-problems-ba2d86ac5837)
* [When Not to Use Deep Learning](http://hyperparameter.space/blog/when-not-to-use-deep-learning/) \| Try reading it again after learning deep learning to see if I can understand the whole article.
* [Jupyter notebooks for the code samples of the book "Deep Learning with Python"](https://github.com/fchollet/deep-learning-with-python-notebooks)
* [Deep Learning Demystified](http://brohrer.github.io/deep_learning_demystified.html)
* [A Non-Expert’s Guide to Image Segmentation Using Deep Neural Nets](https://medium.com/@hanrelan/a-non-experts-guide-to-image-segmentation-using-deep-neural-nets-dda5022f6282)
* [新年新起点，请收下这份诚意满满的“深度学习”教程](http://36kr.com/p/5063385.html)
* [30 Questions to test a Data Scientist on Deep Learning \(Solution – Skill test, July 2017\)](https://www.analyticsvidhya.com/blog/2017/08/skilltest-deep-learning/)
* [Neural networks and deep learning \(book\)](http://neuralnetworksanddeeplearning.com/)
* [Deep Learning : What & Why ?](https://codeburst.io/deep-learning-what-why-dd77d432f182)
* [Tel Aviv Deep Learning Bootcamp](https://github.com/QuantScientist/Deep-Learning-Boot-Camp)
* [colah's blog](http://colah.github.io/)
* [What you need to do deep learning](http://www.fast.ai/2017/11/16/what-you-need/)
* [Deep Learning for Developers: Tools You Can Use to Code Neural Networks on Day 1](https://medium.freecodecamp.org/deep-learning-for-developers-tools-you-can-use-to-code-neural-networks-on-day-1-34c4435ae6b)
* [Deep Learning Glossary – WildML](http://www.wildml.com/deep-learning-glossary/)
* [Deep Learning Tips and Tricks – Towards Data Science](https://towardsdatascience.com/deep-learning-tips-and-tricks-1ef708ec5f53)
* [无需数学背景，读懂 ResNet、Inception 和 Xception 三大变革性架构](https://www.jiqizhixin.com/articles/2017-08-19-4)
* [Introduction to neural network by a16z](http://aiplaybook.a16z.com/docs/guides/dl-architectures)
* [Welch Labs introduction to neural networks](https://www.youtube.com/watch?v=bxe2T-V8XRs), a handful of 4-5 minute, well done videos per a16z
* [A Neural Network in 11 lines of Python \(Part 1\)](http://iamtrask.github.io/2015/07/12/basic-python-network/)
* [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/convolutional-networks/)
* [Google's AI Chief Geoffrey Hinton How Neural Networks Really Work](https://www.youtube.com/watch?v=EInQoVLg_UY)
* [Deep Learning: Intelligence from Big Data](https://www.youtube.com/watch?v=czLI3oLDe8M)
* [Hyperparameter optimization for Neural Networks](http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html)
* [10 Common Software Architectural Patterns in a nutshell](https://medium.com/towards-data-science/10-common-software-architectural-patterns-in-a-nutshell-a0b47a1e9013)
* [An Intuitive Guide to Deep Network Architectures](https://medium.com/towards-data-science/an-intuitive-guide-to-deep-network-architectures-65fdc477db41)
* [But what _is_ a Neural Network? \| Deep learning, Part 1](https://www.youtube.com/watch?v=aircAruvnKk)
* [Selecting the number of neurons in the hidden layer of a neural network](http://firsttimeprogrammer.blogspot.jp/2015/09/selecting-number-of-neurons-in-hidden.html)
* [一图看尽深度学习架构谱系](https://www.jiqizhixin.com/articles/2017-11-07-3)
* [Neural Networks Demystified](https://www.youtube.com/watch?v=bxe2T-V8XRs)
* [Everything you need to know about Neural Networks](https://hackernoon.com/everything-you-need-to-know-about-neural-networks-8988c3ee4491)
* [Neural Networks for Machine Learning by Geoffrey Hinton](https://www.coursera.org/learn/neural-networks)
* [Setting the learning rate of your neural network.](https://www.jeremyjordan.me/nn-learning-rate/)
* [Epoch vs Batch Size vs Iterations – Towards Data Science](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)
* [My Neural Network isn't working! What should I do?](http://theorangeduck.com/page/neural-network-not-working)
* [quark0/darts: Differentiable architecture search for convolutional and recurrent networks](https://github.com/quark0/darts)
* [An Interactive Node-Link Visualization of Convolutional Neural Networks](http://www.cs.cmu.edu/~aharley/vis/)



## Best Practices

{% embed data="{\"url\":\"https://mobile.twitter.com/math\_rachel/status/1013249330583498752\",\"type\":\"rich\",\"title\":\"Rachel Thomas on Twitter\",\"description\":\"These are important points! The fastai library handles 2, 3, & 4 for you, and we teach 1 as a key technique in the course: https://t.co/y1OKto7666— Rachel Thomas \(@math\_rachel\) July 1, 2018\\n\\n\",\"icon\":{\"type\":\"icon\",\"url\":\"https://abs.twimg.com/icons/apple-touch-icon-192x192.png\",\"width\":192,\"height\":192,\"aspectRatio\":1},\"embed\":{\"type\":\"app\",\"html\":\"<blockquote class=\\\"twitter-tweet\\\" data-dnt=\\\"true\\\" align=\\\"center\\\"><p lang=\\\"en\\\" dir=\\\"ltr\\\">These are important points! The fastai library handles 2, 3, &amp; 4 for you, and we teach 1 as a key technique in the course: <a href=\\\"https://t.co/y1OKto7666\\\">https://t.co/y1OKto7666</a></p>&mdash; Rachel Thomas \(@math\_rachel\) <a href=\\\"https://twitter.com/math\_rachel/status/1013249330583498752?ref\_src=twsrc%5Etfw\\\">July 1, 2018</a></blockquote>\\n<script async src=\\\"https://platform.twitter.com/widgets.js\\\" charset=\\\"utf-8\\\"></script>\\n\",\"maxWidth\":550,\"aspectRatio\":1}}" %}



#### What to do with overfitting per fast.ai

1. Add more data
2. Use data augmentation
3. Use architectures that generalize well
4. Add regularization
5. Reduce architecture complexity

[NanoNets : How to use Deep Learning when you have Limited Data](https://medium.com/nanonets/nanonets-how-to-use-deep-learning-when-you-have-limited-data-f68c0b512cab)

[Data Augmentation \| How to use Deep Learning when you have Limited Data — Part 2](https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced)

[MIT 6.S191: Introduction to Deep Learning](http://introtodeeplearning.com/)

[jiqizhixin/ML-Tutorial-Experiment: Coding the Machine Learning Tutorial for Learning to Learn](https://github.com/jiqizhixin/ML-Tutorial-Experiment)

[How to start a Deep Learning project? – Jonathan Hui – Medium](https://medium.com/@jonathan_hui/how-to-start-a-deep-learning-project-d9e1db90fa72)

[Another data science student's blog – The 1cycle policy](https://sgugger.github.io/the-1cycle-policy.html)

[Deep Learning A-Z™: Hands-On Artificial Neural Networks \| Udemy](https://www.udemy.com/deeplearning/?siteID=QZaBth_yPOQ-yCbCBqQJOu918gLAuLj5uQ&LSNPUBID=QZaBth/yPOQ)

[From Perceptron to Deep Neural Nets – Becoming Human: Artificial Intelligence Magazine](https://becominghuman.ai/from-perceptron-to-deep-neural-nets-504b8ff616e)

[10 Audio Processing Projects to start with Deep Learning Applications](https://www.analyticsvidhya.com/blog/2018/01/10-audio-processing-projects-applications/)

[Stochastic Weight Averaging — a New Way to Get State of the Art Results in Deep Learning](https://towardsdatascience.com/stochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a)

[Applied Deep Learning - Part 2: Real World Case Studies](https://towardsdatascience.com/applied-deep-learning-part-2-real-world-case-studies-1bb4b142a585)

## Understand

* [How to build your own Neural Network from scratch in Python](https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6)
* [Backpropagation demo](https://google-developers.appspot.com/machine-learning/crash-course/backprop-scroll/)
* [How to build your own Neural Network from scratch in Python](https://towardsdatascience.com/how-to-build-your-own-neural-network-from-scratch-in-python-68998a08e4f6)

## Blackbox

[  
Visualising Activation Functions in Neural Networks](https://dashee87.github.io/data%20science/deep%20learning/visualising-activation-functions-in-neural-networks/)

[Activation Functions: Neural Networks](https://medium.com/towards-data-science/activation-functions-neural-networks-1cbd9f8d91d6)

[What is backpropagation and what is it actually doing? \| Deep learning, chapter 3](https://www.youtube.com/watch?v=Ilg3gGewQ5U)

[Understanding Activation Functions in Deep Learning](https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/)

[Visualising Activation Functions in Neural Networks](https://dashee87.github.io/data%20science/deep%20learning/visualising-activation-functions-in-neural-networks/)

[Secret Sauce behind the beauty of Deep Learning: Beginners guide to Activation Functions](https://towardsdatascience.com/secret-sauce-behind-the-beauty-of-deep-learning-beginners-guide-to-activation-functions-a8e23a57d046)

[Understanding objective functions in neural networks.](https://towardsdatascience.com/understanding-objective-functions-in-neural-networks-d217cb068138)

Neural nets really like standardized variables. \(per Jeremy\)



#### Examples of what can be tried to improve the model \(From Machine Learning Yearning\)

> * Get more data: Collect more pictures of cats.
> * Collect a more diverse training set. For example, pictures of cats in unusual positions; cats   with unusual coloration; pictures shot with a variety of camera settings; ….
> * Train the algorithm longer, by running more gradient descent iterations.
> * Try a bigger neural network, with more layers/hidden units/parameters.
> * Try a smaller neural network.
> * Try adding regularization \(such as L2 regularization\).
> * Change the neural network architecture \(activation function, number of hidden units, etc.\)

> If you choose well among these possible directions, you’ll build the leading cat picture platform, and lead your company to success. If you choose poorly, you might waste months.



Softmax likes picking one class. So it is ridiculous to use it for multi-label classification. \(per Jeremy\)

Batch Normalization helps your network learn faster by “smoothing” the values at various stages in the stack. Exactly why this works is seemingly not well-understood yet, but it has the effect of helping your network converge much faster, meaning it achieves higher accuracy with less training, or higher accuracy after the same amount of training, often dramatically so.

## Training

* [Stochastic Weight Averaging — a New Way to Get State of the Art Results in Deep Learning](https://towardsdatascience.com/stochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a)
* [李飞飞高徒Andrej Karpathy提醒你，小心搭建神经网络的六个坑 \| 机器之心](https://www.jiqizhixin.com/articles/2018-07-03-19)
* [AdamW and Super-convergence is now the fastest way to train neural nets · fast.ai](http://www.fast.ai/2018/07/02/adam-weight-decay/)

## Gradient Descent

{% embed data="{\"url\":\"https://www.dropbox.com/s/ekee45cl4tel2x2/sgd\_bad.gif?dl=0\",\"type\":\"link\",\"title\":\"sgd\_bad.gif\",\"description\":\"Shared with Dropbox\",\"icon\":{\"type\":\"icon\",\"url\":\"https://cfl.dropboxstatic.com/static/images/logo\_catalog/dropbox\_webclip\_152\_m1-vflU0bwfQ.png\",\"width\":152,\"height\":152,\"aspectRatio\":1},\"thumbnail\":{\"type\":\"thumbnail\",\"url\":\"https://www.dropbox.com/temp\_thumb\_from\_token/s/ekee45cl4tel2x2?preserve\_transparency=False&size=1024x1024&size\_mode=2\",\"width\":1024,\"height\":1024,\"aspectRatio\":1}}" %}

{% embed data="{\"url\":\"https://www.dropbox.com/s/5ptildzwybdtjp3/sgd.gif?dl=0\",\"type\":\"link\",\"title\":\"sgd.gif\",\"description\":\"Shared with Dropbox\",\"icon\":{\"type\":\"icon\",\"url\":\"https://cfl.dropboxstatic.com/static/images/logo\_catalog/dropbox\_webclip\_152\_m1-vflU0bwfQ.png\",\"width\":152,\"height\":152,\"aspectRatio\":1},\"thumbnail\":{\"type\":\"thumbnail\",\"url\":\"https://www.dropbox.com/temp\_thumb\_from\_token/s/5ptildzwybdtjp3?preserve\_transparency=False&size=1024x1024&size\_mode=2\",\"width\":1024,\"height\":1024,\"aspectRatio\":1}}" %}

* If the learning rate is too large \(0.01\), the cost may oscillate up and down. It may even diverge \(refer to the graph above\).
* A lower cost doesn't mean a better model. You have to check if there is possibly overfitting. It happens when the training accuracy is a lot higher than the test accuracy.
* In deep learning, we usually recommend that you:
* Choose the learning rate that better minimizes the cost function.
* If your model overfits, use other techniques to reduce overfitting.
* Complexity of machine learning comes from the data rather than from the lines of code.
* Intuitions from one domain or from one application area often do not transfer to other application areas.
* Applied deep learning is a very iterative process where you just have to go around this cycle many times to hopefully find a good choice of network for your application.



## Initialization

* Random initialization is used to break symmetry and make sure different hidden units can learn different things.
  * The weights ​$W^{\[l\]}​$ should be initialized randomly to break symmetry.
  * It is however okay to initialize the biases $b^{\[l\]}​$​ to zeros. Symmetry is still broken so long as ​$W^{\[l\]}​$ is initialized randomly.
* Initializing weights to very large random values does not work well.
* Hopefully initializing with small random values does better. The important question is: how small should be these random values be? Lets find out in the next part!
* Different initializations lead to different results
* He initialization works well for networks with ReLU activations.



## Deal with high variance

### Regularization

* Regularization will help you reduce overfitting.
* Regularization will drive your weights to lower values.
* L2 regularization and Dropout are two very effective regularization techniques.

#### L2-regularization

* What is L2-regularization actually doing? L2-regularization relies on the assumption that a model with small weights is simpler than a model with large weights. Thus, by penalizing the square values of the weights in the cost function you drive all the weights to smaller values. It becomes too costly for the cost to have large weights! This leads to a smoother model in which the output changes more slowly as the input changes.
* the implications of L2-regularization on:
  * The cost computation: A regularization term is added to the cost
  * The backpropagation function: There are extra terms in the gradients with respect to weight matrices
  * Weights end up smaller \("weight decay"\): Weights are pushed to smaller values.

### Dropout

* Dropout is a regularization technique.
* You only use dropout during training. Don't use dropout \(randomly eliminate nodes\) during test time.
* Apply dropout both during forward and backward propagation.
* During training time, divide each dropout layer by keep\_prob to keep the same expected value for the activations. For example, if keep\_prob is 0.5, then we will on average shut down half the nodes, so the output will be scaled by 0.5 since only the remaining half are contributing to the solution. Dividing by 0.5 is equivalent to multiplying by 2. Hence, the output now has the same expected value. You can check that this works even when keep\_prob is other values than 0.5.

## Gradient Checking

* Gradient checking verifies closeness between the gradients from backpropagation and the numerical approximation of the gradient \(computed using forward propagation\).
* Gradient checking is slow, so we don't run it in every iteration of training. You would usually run it only to make sure your code is correct, then turn it off and use backprop for the actual learning process.

## Mini-batch Gradient Descent and Stochastic Gradient Descent \(SGD\)

* The difference between gradient descent, mini-batch gradient descent and stochastic gradient descent is the number of examples you use to perform one update step.
* You have to tune a learning rate hyperparameter ​.
* With a well-turned mini-batch size, usually it outperforms either gradient descent or stochastic gradient descent \(particularly when the training set is large\).
* Shuffling and Partitioning are the two steps required to build mini-batches
* Powers of two are often chosen to be the mini-batch size, e.g., 16, 32, 64, 128.

## Gradient descent with Momentum

* Common values for ​ range from 0.8 to 0.999. If you don't feel inclined to tune this, ​$\beta = 0.9​$ is often a reasonable default.
* Momentum takes past gradients into account to smooth out the steps of gradient descent. It can be applied with batch gradient descent, mini-batch gradient descent or stochastic gradient descent.
* You have to tune a momentum hyperparameter ​$\beta​$ and a learning rate ​$\alpha​$.

## Understand the algorithm

* [How to build a three-layer neural network from scratch](https://medium.freecodecamp.org/building-a-3-layer-neural-network-from-scratch-99239c4af5d3)
* [How to Make a Prediction - Intro to Deep Learning \#1 - YouTube](https://www.youtube.com/watch?v=vOppzHpvTiQ&t=274s)

