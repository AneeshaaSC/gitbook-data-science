# Deep Learning in General

Neural nets really like standardized variables. \(per Jeremy\)

Softmax likes picking one class. So it is ridiculous to use it for multi-label classification \(per Jeremy\)

If a smaller batch size is used, the gradient is calculated using less number of images so it is less accurate as it is more volatile. You can try to re-run the learning rate finder to see if the best learning rate changed but it shouldn't make a huge difference as the learning rate differ exponentially. \(per Jeremy\)

[Improving the way we work with learning rate. – techburst](https://techburst.io/improving-the-way-we-work-with-learning-rate-5e99554f163b)

[The Cyclical Learning Rate technique // teleported.in](http://teleported.in/posts/cyclic-learning-rate/)

[Exploring Stochastic Gradient Descent with Restarts \(SGDR\)](https://medium.com/38th-street-studios/exploring-stochastic-gradient-descent-with-restarts-sgdr-fa206c38a74e)

[Notes from Coursera Deep Learning courses by Andrew Ng by Tess Ferrandez](https://www.dropbox.com/s/up1iifq2uvgg0zb/paper-180305120451.pdf?dl=0)

[Estimating an Optimal Learning Rate For a Deep Neural Network](https://towardsdatascience.com/estimating-optimal-learning-rate-for-a-deep-neural-network-ce32f2556ce0)



* [https://quid.com/feed/how-quid-uses-deep-learning-with-small-data](https://quid.com/feed/how-quid-uses-deep-learning-with-small-data) Recommended by Jeremey
* [https://medium.com/towards-data-science/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c](https://medium.com/towards-data-science/linear-algebra-cheat-sheet-for-deep-learning-cd67aba4526c) Recommended by Jeremey
* [Three key learnings helped Merantix when applying deep learning to real-world problems](https://medium.com/merantix/applying-deep-learning-to-real-world-problems-ba2d86ac5837)
* [When Not to Use Deep Learning](http://hyperparameter.space/blog/when-not-to-use-deep-learning/) \| Try reading it again after learning deep learning to see if I can understand the whole article.
* [Jupyter notebooks for the code samples of the book "Deep Learning with Python"](https://github.com/fchollet/deep-learning-with-python-notebooks)
* [Deep Learning Demystified](http://brohrer.github.io/deep_learning_demystified.html)
* [A Non-Expert’s Guide to Image Segmentation Using Deep Neural Nets](https://medium.com/@hanrelan/a-non-experts-guide-to-image-segmentation-using-deep-neural-nets-dda5022f6282)
* [新年新起点，请收下这份诚意满满的“深度学习”教程](http://36kr.com/p/5063385.html)
* [30 Questions to test a Data Scientist on Deep Learning \(Solution – Skill test, July 2017\)](https://www.analyticsvidhya.com/blog/2017/08/skilltest-deep-learning/)
* [Neural networks and deep learning \(book\)](http://neuralnetworksanddeeplearning.com/)
* [Deep Learning : What & Why ?](https://codeburst.io/deep-learning-what-why-dd77d432f182)
* [Tel Aviv Deep Learning Bootcamp](https://github.com/QuantScientist/Deep-Learning-Boot-Camp)
* [colah's blog](http://colah.github.io/)
* [What you need to do deep learning](http://www.fast.ai/2017/11/16/what-you-need/)
* [Deep Learning for Developers: Tools You Can Use to Code Neural Networks on Day 1](https://medium.freecodecamp.org/deep-learning-for-developers-tools-you-can-use-to-code-neural-networks-on-day-1-34c4435ae6b)
* [Deep Learning Glossary – WildML](http://www.wildml.com/deep-learning-glossary/)





* [无需数学背景，读懂 ResNet、Inception 和 Xception 三大变革性架构](https://www.jiqizhixin.com/articles/2017-08-19-4)
* [Introduction to neural network by a16z](http://aiplaybook.a16z.com/docs/guides/dl-architectures)
* [Welch Labs introduction to neural networks](https://www.youtube.com/watch?v=bxe2T-V8XRs), a handful of 4-5 minute, well done videos per a16z
* [A Neural Network in 11 lines of Python \(Part 1\)](http://iamtrask.github.io/2015/07/12/basic-python-network/)
* [CS231n Convolutional Neural Networks for Visual Recognition](http://cs231n.github.io/convolutional-networks/)
* [Google's AI Chief Geoffrey Hinton How Neural Networks Really Work](https://www.youtube.com/watch?v=EInQoVLg_UY)
* [Deep Learning: Intelligence from Big Data](https://www.youtube.com/watch?v=czLI3oLDe8M)
* [Hyperparameter optimization for Neural Networks](http://neupy.com/2016/12/17/hyperparameter_optimization_for_neural_networks.html)
* [10 Common Software Architectural Patterns in a nutshell](https://medium.com/towards-data-science/10-common-software-architectural-patterns-in-a-nutshell-a0b47a1e9013)
* [An Intuitive Guide to Deep Network Architectures](https://medium.com/towards-data-science/an-intuitive-guide-to-deep-network-architectures-65fdc477db41)
* [But what _is_ a Neural Network? \| Deep learning, Part 1](https://www.youtube.com/watch?v=aircAruvnKk)
* [Selecting the number of neurons in the hidden layer of a neural network](http://firsttimeprogrammer.blogspot.jp/2015/09/selecting-number-of-neurons-in-hidden.html)
* [一图看尽深度学习架构谱系](https://www.jiqizhixin.com/articles/2017-11-07-3)
* [Neural Networks Demystified](https://www.youtube.com/watch?v=bxe2T-V8XRs)
* [Everything you need to know about Neural Networks](https://hackernoon.com/everything-you-need-to-know-about-neural-networks-8988c3ee4491)
* [Neural Networks for Machine Learning by Geoffrey Hinton](https://www.coursera.org/learn/neural-networks)

[Epoch vs Batch Size vs Iterations – Towards Data Science](https://towardsdatascience.com/epoch-vs-iterations-vs-batch-size-4dfb9c7ce9c9)

[My Neural Network isn't working! What should I do?](http://theorangeduck.com/page/neural-network-not-working)



#### What to do with overfitting per fast.ai

1. Add more data
2. Use data augmentation
3. Use architectures that generalize well
4. Add regularization
5. Reduce architecture complexity

[NanoNets : How to use Deep Learning when you have Limited Data](https://medium.com/nanonets/nanonets-how-to-use-deep-learning-when-you-have-limited-data-f68c0b512cab)

[Data Augmentation \| How to use Deep Learning when you have Limited Data — Part 2](https://medium.com/nanonets/how-to-use-deep-learning-when-you-have-limited-data-part-2-data-augmentation-c26971dc8ced)

[MIT 6.S191: Introduction to Deep Learning](http://introtodeeplearning.com/)

[jiqizhixin/ML-Tutorial-Experiment: Coding the Machine Learning Tutorial for Learning to Learn](https://github.com/jiqizhixin/ML-Tutorial-Experiment)

[How to start a Deep Learning project? – Jonathan Hui – Medium](https://medium.com/@jonathan_hui/how-to-start-a-deep-learning-project-d9e1db90fa72)

[Another data science student's blog – The 1cycle policy](https://sgugger.github.io/the-1cycle-policy.html)

[Deep Learning A-Z™: Hands-On Artificial Neural Networks \| Udemy](https://www.udemy.com/deeplearning/?siteID=QZaBth_yPOQ-yCbCBqQJOu918gLAuLj5uQ&LSNPUBID=QZaBth/yPOQ)

[From Perceptron to Deep Neural Nets – Becoming Human: Artificial Intelligence Magazine](https://becominghuman.ai/from-perceptron-to-deep-neural-nets-504b8ff616e)

[10 Audio Processing Projects to start with Deep Learning Applications](https://www.analyticsvidhya.com/blog/2018/01/10-audio-processing-projects-applications/)

[Stochastic Weight Averaging — a New Way to Get State of the Art Results in Deep Learning](https://towardsdatascience.com/stochastic-weight-averaging-a-new-way-to-get-state-of-the-art-results-in-deep-learning-c639ccf36a)

[Applied Deep Learning - Part 2: Real World Case Studies](https://towardsdatascience.com/applied-deep-learning-part-2-real-world-case-studies-1bb4b142a585)



## Blackbox

[  
Visualising Activation Functions in Neural Networks](https://dashee87.github.io/data%20science/deep%20learning/visualising-activation-functions-in-neural-networks/)

[Activation Functions: Neural Networks](https://medium.com/towards-data-science/activation-functions-neural-networks-1cbd9f8d91d6)

[What is backpropagation and what is it actually doing? \| Deep learning, chapter 3](https://www.youtube.com/watch?v=Ilg3gGewQ5U)

[Understanding Activation Functions in Deep Learning](https://www.learnopencv.com/understanding-activation-functions-in-deep-learning/)

[Visualising Activation Functions in Neural Networks](https://dashee87.github.io/data%20science/deep%20learning/visualising-activation-functions-in-neural-networks/)

[Secret Sauce behind the beauty of Deep Learning: Beginners guide to Activation Functions](https://towardsdatascience.com/secret-sauce-behind-the-beauty-of-deep-learning-beginners-guide-to-activation-functions-a8e23a57d046)

[Understanding objective functions in neural networks.](https://towardsdatascience.com/understanding-objective-functions-in-neural-networks-d217cb068138)





#### Examples of what can be tried to improve the model \(From Machine Learning Yearning\)

> * Get more data: Collect more pictures of cats.
> * Collect a more diverse training set. For example, pictures of cats in unusual positions; cats   with unusual coloration; pictures shot with a variety of camera settings; ….
> * Train the algorithm longer, by running more gradient descent iterations.
> * Try a bigger neural network, with more layers/hidden units/parameters.
> * Try a smaller neural network.
> * Try adding regularization \(such as L2 regularization\).
> * Change the neural network architecture \(activation function, number of hidden units, etc.\)

> If you choose well among these possible directions, you’ll build the leading cat picture platform, and lead your company to success. If you choose poorly, you might waste months.

