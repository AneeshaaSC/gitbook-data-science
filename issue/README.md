# Issue

* [**Toward ethical, transparent and fair AI/ML: a critical reading list**](https://medium.com/@eirinimalliaraki/toward-ethical-transparent-and-fair-ai-ml-a-critical-reading-list-d950e70a70ea)
* [Towards fairness in ML with adversarial networks](https://blog.godatadriven.com/fairness-in-ml)
* [natural language processing blog: Many opportunities for discrimination in deploying machine learning systems](https://nlpers.blogspot.com/2018/06/many-opportunities-for-discimination-in.html)
* [Explain yourself, machine. Producing simple text descriptions for AI interpretability. – Luke Oakden-Rayner](https://lukeoakdenrayner.wordpress.com/2018/06/05/explain-yourself-machine-producing-simple-text-descriptions-for-ai-interpretability/)
* [Managing risk in machine learning models - O'Reilly Media](https://www.oreilly.com/ideas/managing-risk-in-machine-learning-models)
* [We need to build machine learning tools to augment machine learning engineers - O'Reilly Media](https://www.oreilly.com/ideas/we-need-to-build-machine-learning-tools-to-augment-machine-learning-engineers)
* [Scientific debt – Variance Explained](http://varianceexplained.org/r/scientific-debt/)
* [Data Ethics Framework - GOV.UK](https://www.gov.uk/government/publications/data-ethics-framework/data-ethics-framework)
* [**Attacks against machine learning — an overview**](https://elie.net/blog/ai/attacks-against-machine-learning-an-overview)
* [Challenges faced while training an AI to combat abuse](https://elie.net/blog/ai/challenges-faced-while-training-an-ai-to-combat-abuse)
* [natural language processing blog: Many opportunities for discrimination in deploying machine learning systems](https://nlpers.blogspot.com/2018/06/many-opportunities-for-discimination-in.html)

## Interpretability

* [tensorflow/lucid: A collection of infrastructure and tools for research in neural network interpretability.](https://github.com/tensorflow/lucid)
* [TeamHG-Memex/eli5: A library for debugging/inspecting machine learning classifiers and explaining their predictions](https://github.com/TeamHG-Memex/eli5)
* [Human Interpretable Machine Learning \(Part 1\) — The Need and Importance of Model Interpretation](https://towardsdatascience.com/human-interpretable-machine-learning-part-1-the-need-and-importance-of-model-interpretation-2ed758f5f476)
* [Testing machine learning interpretability techniques - O'Reilly Media](https://www.oreilly.com/ideas/testing-machine-learning-interpretability-techniques)
* [**lopusz/awesome-interpretable-machine-learning**](https://github.com/lopusz/awesome-interpretable-machine-learning)
* [可微编程：打开深度学习的黑盒子](https://zhuanlan.zhihu.com/p/37863641)

##  Fairness

* [Fairness in Machine Learning with PyTorch](https://blog.godatadriven.com/fairness-in-pytorch)
* [The Trouble with Bias - NIPS 2017 Keynote - Kate Crawford \#NIPS2017 - YouTube](https://www.youtube.com/watch?v=fMym_BKWQzk)
* [Debugging data: Microsoft researchers look at ways to train AI systems to reflect the real world - The AI Blog](https://blogs.microsoft.com/ai/debugging-data-microsoft-researchers-look-ways-train-ai-systems-reflect-real-world/)
* [What does it really mean for an algorithm to be biased?](https://thegradient.pub/ai-bias/)
* [Delayed Impact of Fair Machine Learning – The Berkeley Artificial Intelligence Research Blog](http://bair.berkeley.edu/blog/2018/05/17/delayed-impact/)
* [natural language processing blog: Many opportunities for discrimination in deploying machine learning systems](https://nlpers.blogspot.com/2018/06/many-opportunities-for-discimination-in.html)
* [Bias detectives: the researchers striving to make algorithms fair](https://www.nature.com/articles/d41586-018-05469-3)

